# ğŸ§  RAG System with Google Gemma-3 + FastAPI + Streamlit

A full-stack **Retrieval-Augmented Generation (RAG)** project powered by **Google's latest Gemma-3 LLM**, this system enables intelligent document-based question answering and summarization using local LLM inference. Built for both learning and showcasing modern AI engineering, this project includes a **FastAPI backend** and **Streamlit frontend**, and integrates Chroma vector store with LangChain for document retrieval.

---

## ğŸ“Œ Table of Contents
- [ğŸš€ Introduction](#-introduction)
- [ğŸ§° Tech Stack](#-tech-stack)
- [ğŸ“ Project Structure](#-project-structure)
- [âš™ï¸ Installation](#-installation)
- [â–¶ï¸ How to Run](#-how-to-run)
- [ğŸ’¡ Features](#-features)
- [ğŸ“š Applications](#-applications)
- [ğŸ“¸ Screenshots](#-screenshots)

---

## ğŸš€ Introduction

This project is a practical implementation of a **local RAG (Retrieval-Augmented Generation) pipeline** using Google's recently released **Gemma-3 LLM**. It combines:

- ğŸ“„ Document ingestion (PDF, TXT, DOCX, CSV)
- ğŸ” Vector-based retrieval (using LangChain + ChromaDB)
- ğŸ§  Contextual answer generation & summarization (using Gemma-3)
- ğŸŒ Full-stack app with FastAPI + Streamlit

### âœ¨ Why This Project?
This was created as an **AI engineering experiment** to:
- Test the performance of Google's **Gemma-3 LLM (2025)** locally
- Build a scalable pipeline for enterprise-style document QA
- Offer a hands-on learning example of RAG systems for developers

---

## ğŸ§° Tech Stack

| Layer         | Tools/Frameworks                                   |
|--------------|----------------------------------------------------|
| **Frontend** | Streamlit                                           |
| **Backend**  | FastAPI, Uvicorn                                    |
| **LLM**      | Google Gemma-3 via HuggingFace Transformers         |
| **Vector DB**| ChromaDB (via LangChain Chroma)                     |
| **Embeddings**| SentenceTransformers (`all-MiniLM-L6-v2`)         |
| **Utilities**| dotenv, docx2txt, pypdf, python-docx                |

---

## ğŸ“ Project Structure
```
RAG-GEMMA3/
â”œâ”€â”€ Backend/
â”‚   â”œâ”€â”€ main.py                # FastAPI backend
â”‚   â”œâ”€â”€ main_4bit.py           # FastAPI backend using 4bit quantized Gemma-3
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ Frontend/
â”‚   â””â”€â”€ app.py                 # Streamlit interface for users
â”œâ”€â”€ requirements.txt           # All dependencies
â”œâ”€â”€ run_project.py             # One-click launcher (venv + server start)
â”œâ”€â”€ .gitignore
â””â”€â”€ .env                       # API keys or model paths (optional)
```

---

## âš™ï¸ Installation

### ğŸ”§ Prerequisites
- âœ… Python 3.11 (strictly required)
- âœ… Git installed

### ğŸ› ï¸ Steps to Setup
```bash
# 1. Clone the repository
git clone https://github.com/Kartik-A-1820/RAG-GEMMA3.git
cd RAG-GEMMA3

# 2. Run the auto setup script (creates venv, installs, launches everything)
python run_project.py
```

> ğŸš€ This will:
> - Create `venv/`
> - Install all required packages
> - Start backend on `http://localhost:8000`
> - Start frontend on `http://localhost:8501`

---

## â–¶ï¸ How to Run (Manual Option)
If you want to run manually:
```bash
# 1. Clone the repository
git clone https://github.com/Kartik-A-1820/RAG-GEMMA3.git
cd RAG-GEMMA3

# 2. Create a virtual environment
python -m venv venv

# 3. Activate the virtual environment
# For Windows:
venv\Scripts\activate
# For Linux/macOS:
source venv/bin/activate

# 4. Install dependencies
pip install -r requirements.txt

# 5. Start the FastAPI backend (in one terminal)
uvicorn Backend.main_4bit:app --host 0.0.0.0 --port 8000 --reload

# 6. Start the Streamlit frontend (in another terminal)
streamlit run Frontend/app.py
```

---

## ğŸ’¡ Features

- âœ… **Document Ingestion**
  - Upload PDF, TXT, DOCX, CSV
  - Content is split and vectorized

- ğŸ” **Semantic Search & QA**
  - Uses Chroma for fast retrieval
  - Top relevant chunks used for context

- ğŸ§  **Answer Generation**
  - Powered by quantized Gemma-3 LLM
  - Returns grounded answers with source references

- ğŸ“ **PDF Summarization**
  - Summarizes long documents into concise texts
  - Chunked batching with final merged summary

- ğŸ§ª **One-click launcher**
  - `run_project.py` simplifies setup and execution

---

## ğŸ“š Applications

- ğŸ“Š **Enterprise Search**: Build intelligent internal knowledge bases
- ğŸ“š **Academic Summarizers**: Convert papers/books into key points
- ğŸ“„ **Legal Document QA**: Upload policies/contracts and ask questions
- ğŸ§  **AI-Powered Assistants**: Build a base for offline personal AI
- ğŸ’¼ **Resume/Job-Description Matching**: Semantic profile matching

---

## ğŸ“¸ Screenshots

### ğŸ  Home Page
![Home](imgs/1.png)

### ğŸ“„ Document Ingestion
![Document Upload](imgs/2.png)

### â“ Answering Queries with References
![Query Response](imgs/3.png)

### ğŸ“ Summarizing PDF
![PDF Summary](imgs/4.png)

---

## ğŸ™Œ Credits
- [Google AI](https://ai.google.dev/gemma) for releasing Gemma-3
- [LangChain](https://www.langchain.com)
- [HuggingFace Transformers](https://huggingface.co/transformers)
- [Chroma Vector DB](https://www.trychroma.com)

---

> ğŸ’¬ Have questions or want to collaborate? Reach out at [LinkedIn](https://www.linkedin.com/in/kartik-anagawadi-4b33a81b6/)

